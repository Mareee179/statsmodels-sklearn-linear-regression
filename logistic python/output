michaeljgrogan@michaeljgrogan-HP-Notebook:~$ python3
Python 3.6.6 (default, Sep 12 2018, 18:26:19) 
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import numpy as np
>>> import statsmodels.api as sm
>>> import statsmodels.formula.api as smf
>>> import pandas as pd
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.model_selection import train_test_split
>>> import os;
>>> path="/home/michaeljgrogan/Documents/a_documents/computing/data science/datasets"
>>> os.chdir(path)
>>> os.getcwd()
'/home/michaeljgrogan/Documents/a_documents/computing/data science/datasets'
>>> 
>>> variables=pd.read_csv('dividendinfo.csv')
>>> dividend=variables['dividend']
>>> freecashflow_pershare=variables['fcfps']
>>> earnings_growth=variables['earnings_growth']
>>> debt_to_equity=variables['de']
>>> mcap=variables['mcap']
>>> current_ratio=variables['current_ratio']
>>> 
>>> # debt_to_equity.shape
... 
>>> y=dividend
>>> x=np.column_stack((freecashflow_pershare,earnings_growth,debt_to_equity,mcap,current_ratio))
>>> x=sm.add_constant(x,prepend=True)
>>> 
>>> x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0)
>>> logreg=LogisticRegression().fit(x_train,y_train)
>>> logreg
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)
>>> print("Training set score: {:.3f}".format(logreg.score(x_train,y_train)))
Training set score: 0.920
>>> print("Test set score: {:.3f}".format(logreg.score(x_test,y_test)))
Test set score: 0.960
>>> 
>>> import statsmodels.api as sm
>>> logit_model=sm.Logit(y_train,x_train)
>>> result=logit_model.fit()
Optimization terminated successfully.
         Current function value: 0.111698
         Iterations 10
>>> print(result.summary())
                           Logit Regression Results                           
==============================================================================
Dep. Variable:               dividend   No. Observations:                  150
Model:                          Logit   Df Residuals:                      144
Method:                           MLE   Df Model:                            5
Date:                Sun, 07 Oct 2018   Pseudo R-squ.:                  0.8385
Time:                        17:31:21   Log-Likelihood:                -16.755
converged:                       True   LL-Null:                       -103.76
                                        LLR p-value:                 1.018e-35
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const        -21.1447      6.084     -3.475      0.001     -33.069      -9.220
x1             1.7848      0.578      3.087      0.002       0.652       2.918
x2             0.1035      0.035      2.952      0.003       0.035       0.172
x3            -0.9209      0.538     -1.713      0.087      -1.975       0.133
x4             0.0246      0.007      3.476      0.001       0.011       0.039
x5             5.2756      1.790      2.948      0.003       1.768       8.783
==============================================================================

Possibly complete quasi-separation: A fraction 0.39 of observations can be
perfectly predicted. This might indicate that there is complete
quasi-separation. In this case some parameters will not be identified.
>>> 
>>> import matplotlib.pyplot as plt
>>> from sklearn.metrics import roc_curve
>>> falsepos,truepos,thresholds=roc_curve(y_test,logreg.decision_function(x_test))
>>> 
>>> plt.plot(falsepos,truepos,label="ROC")
[<matplotlib.lines.Line2D object at 0x7f01d151fa20>]
>>> plt.xlabel("False Positive Rate")
Text(0.5,0,'False Positive Rate')
>>> plt.ylabel("True Positive Rate")
Text(0,0.5,'True Positive Rate')
>>> 
>>> cutoff=np.argmin(np.abs(thresholds))
>>> plt.plot(falsepos[cutoff],truepos[cutoff],'o',markersize=10,label="cutoff",fillstyle="none")
[<matplotlib.lines.Line2D object at 0x7f01d151fc88>]
>>> plt.show()
>>> 
>>> from sklearn import metrics
>>> metrics.auc(falsepos, truepos)
0.9932088285229203
